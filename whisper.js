async function transcribeAudio(audioBlob) {
    if (!(audioBlob instanceof Blob)) {
        throw new Error("El archivo de audio no es un Blob v√°lido.");
    }

    const formData = new FormData();
    formData.append("file", audioBlob, "audio.webm");
    formData.append("model", "whisper-1");
    formData.append("response_format", "text");

    try {
        const response = await fetch("https://api.openai.com/v1/audio/transcriptions", {
            method: "POST",
            headers: {
                "Authorization": `Bearer ${CONFIG.OPENAI_API_KEY}`
            },
            body: formData
        });

        // Verificamos si la respuesta es exitosa
        if (!response.ok) {
            throw new Error(`Error en la API: ${response.status} - ${response.statusText}`);
        }

        const data = await response.json();

        // üõ†Ô∏è Debugging: Imprimir la respuesta de OpenAI de forma clara
        console.log("üîç Respuesta completa de Whisper:\n", JSON.stringify(data, null, 2));

        // Validamos si `text` est√° en la respuesta
        if (data.text) {
            return data.text;
        } else {
            throw new Error("‚ùå La API de Whisper no devolvi√≥ 'text'. Revisa la respuesta en la consola.");
        }
    } catch (error) {
        console.error("üö® Error en la transcripci√≥n de audio:", error);
        return `Error en la transcripci√≥n: ${error.message}`;
    }
}
